# vector_db.py   å°†å›¾æ–‡markdownå†™å…¥å‘é‡æ•°æ®åº“
import sys, os, hashlib, re 
from pathlib import Path
from typing import List, Union
from tqdm import tqdm

# ä½¿ç”¨ Unstructured æ›´å¥½åœ°è§£æ Markdown ä¸­çš„å›¾ç‰‡é“¾æ¥
from langchain_community.document_loaders import PyPDFLoader, UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings

class VectorIngestor:
    def __init__(self, config: dict):
        self.config = config
        self.embeddings = self._load_embedding_model()
        self.vectordb = self._init_vectorstore()

    def _load_embedding_model(self):
        model = self.config["embed_model"]
        base_url = self.config.get("base_url")
        device = self.config.get("device", "cpu")

        if model.startswith("ollama:"):
            model_name = model[7:]
            print(f"[*] ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹: {model_name}")
            return OllamaEmbeddings(model=model_name, base_url=base_url)

        else:
            print(f"[*] ä½¿ç”¨ HuggingFace åµŒå…¥æ¨¡å‹: {model}")
            return HuggingFaceEmbeddings(
                model_name=model,
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True},
            )

    def _init_vectorstore(self) -> Chroma:
        return Chroma(
            collection_name=self.config["collection_name"],
            embedding_function=self.embeddings,
            persist_directory=self.config["store_dir"],
        )

    def load_docs(self, roots: Union[str, Path]) -> List:
        """é€’å½’åŠ è½½ .md æ–‡ä»¶ï¼Œä½¿ç”¨ Unstructured è§£æï¼ˆä¿ç•™å›¾ç‰‡å¼•ç”¨ï¼‰"""
        roots = [Path(r) for r in roots]
        docs = []
        
        for root in roots:
            if not Path(root).exists():
                print(f"[WARN] æ ¹ç›®å½•ä¸å­˜åœ¨: {root}")
                continue
            
            # åŒ¹é… root/*/auto
            for auto_dir in root.glob("*/auto"):
                if not auto_dir.is_dir():
                    print(f"[WARN] è·³è¿‡æ–‡ä»¶ {root}")
                    continue
                
                md_dir = auto_dir  # *.md æ”¾åœ¨ auto/ ç›®å½•ä¸‹
                img_dir = auto_dir / "images"  # å›¾ç‰‡åœ¨ auto/images/
                
                # åŠ è½½æ‰€æœ‰ .md æ–‡ä»¶
                for md_file in md_dir.glob("*.md"):
                    try:
                        loader = UnstructuredMarkdownLoader(str(md_file), mode="elements")
                        loaded_docs = loader.load()

                        for doc in loaded_docs:
                            source_path = str(md_file.resolve())
                            doc.metadata.update({
                                "source_path": source_path,
                                "file_stem": md_file.stem,
                                "file_suffix": md_file.suffix,
                                "file_size": md_file.stat().st_size,
                                "auto_root": str(auto_dir),  # æ ‡è®°æ¥æº
                            })

                            # æå–å›¾ç‰‡å¼•ç”¨ï¼ˆå¦‚: ![](images/solar.png)ï¼‰
                            image_paths = re.findall(r"!\[.*?\]\((.+?)\)", doc.page_content)
                            if image_paths:
                                resolved_images = []
                                for img_ref in image_paths:
                                    img_ref = img_ref.strip()
                                    # å°è¯•è§£æä¸ºç›¸å¯¹äº auto/ ç›®å½•çš„è·¯å¾„
                                    if img_ref.startswith("images/"):
                                        img_full = (auto_dir / img_ref).resolve()
                                        if img_full.exists():
                                            resolved_images.append(str(img_full))

                                if resolved_images:
                                    doc.metadata["image_paths"] = resolved_images

                        docs.extend(loaded_docs)

                    except Exception as e:
                        print(f"[WARN] è§£ææ–‡ä»¶å¤±è´¥ {md_file}: {e}")


                # # å¯é€‰ï¼šä¹Ÿæ”¯æŒ PDFï¼ˆä½†ä¸å¤„ç†å…¶ä¸­å›¾ç‰‡ï¼‰
                # for fp in root.rglob("*.pdf"):
                #     try:
                #         loader = PyPDFLoader(str(fp))
                #         loaded_docs = loader.load()
                #         for doc in loaded_docs:
                #             doc.metadata["source_path"] = str(fp.resolve())
                #             doc.metadata["file_stem"] = fp.stem
                #             doc.metadata["file_suffix"] = ".pdf"
                #         docs.extend(loaded_docs)
                #     except Exception as e:
                #         print(f"[WARN] è·³è¿‡ PDF {fp}: {e}")

        print(f"[*] æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
        return docs

    def split_docs(self, docs: List, chunk_size: int, chunk_overlap: int) -> List:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        chunks = splitter.split_documents(docs)
        print(f"[*] åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks

    @staticmethod
    def make_id(doc) -> str:
        content = doc.page_content.strip()
        sha = hashlib.sha1(content.encode("utf-8")).hexdigest()[:16]
        stem = Path(doc.metadata.get("source_path", "chunk")).stem
        return f"{sha}_{stem}"

    def get_existing_ids(self) -> set:
        try:
            return set(self.vectordb.get()["ids"])
        except Exception as e:
            print(f"[INFO] æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡åº“ï¼Œå°†åˆ›å»ºæ–°çš„: {e}")
            return set()

    def ingest(self):
        raw_docs = self.load_docs(self.config["docs_root"])
        if not raw_docs:
            print("[WARN] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç»ˆæ­¢ç´¢å¼•ã€‚")
            return
        
        chunks = self.split_docs(raw_docs, self.config["chunk_size"], self.config["chunk_overlap"])
        chunk_ids = [self.make_id(chunk) for chunk in chunks]

        existing_ids = self.get_existing_ids()
        to_add = [(chunk, cid) for chunk, cid in zip(chunks, chunk_ids) if cid not in existing_ids]

        if not to_add:
            print("[*] æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦ç´¢å¼•ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return

        print(f"[*] æ–°å¢ {len(to_add)} ä¸ªæ–‡æœ¬å—")

        batch_size = self.config["batch_size"]
        for i in tqdm(range(0, len(to_add), batch_size), desc="ğŸ”„ å†™å…¥å‘é‡åº“"):
            batch = to_add[i:i + batch_size]
            docs, ids = zip(*batch)
            self.vectordb.add_documents(documents=docs, ids=list(ids))

        print(f"[âœ…] ç´¢å¼•å®Œæˆï¼æ–°å¢ {len(to_add)} æ¡ï¼Œå‘é‡åº“å­˜å‚¨äº:\n    {self.config['store_dir']}")




if __name__ =="__main__": 
    # config 
    DEFAULT_CONFIG = {
    "roots": [
        r".\docs.log\zhuanli_RobotFeet",
        r".\docs.log\zhuanli_RobotHand"
    ],
    "store_dir": "./docs.log/chroma_db/.zhuanli_vectdb",
    "collection_name": "multi_domain_knowledge",
    "embed_model": "Qwen/Qwen3-Embedding-0.6B",
    "chunk_size": 480,
    "chunk_overlap": 128,
    "batch_size": 16,
    "base_url": "http://localhost:11434",
    "device": "cuda" if os.environ.get("CUDA_VISIBLE_DEVICES", "") != "" else "cpu",
}
    ingestor = VectorIngestor(DEFAULT_CONFIG)
    ingestor.ingest()
    
      







