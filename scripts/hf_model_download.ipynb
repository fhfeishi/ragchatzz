{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1425c6e4",
   "metadata": {},
   "source": [
    "* **download hf model**  \n",
    "* method-1: snapshot_download \n",
    "\n",
    "<!-- \n",
    "- 纯粹下载模型仓库的所有文件，不会加载模型或初始化任何HuggingFace的高层对象\n",
    "- 用途：适合离线部署、镜像备份、完整复制模型目录。\n",
    "- 输出：返回一个本地路径（通常是缓存目录），你可以手动移动或指定 local_dir。\n",
    "- 不加载模型，不涉及 transformers，因此不会触发模型结构检查或依赖项验证 \n",
    "-->\n",
    "\n",
    "只想下载模型文件（如部署到服务器）：用 snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac8a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from huggingface_hub import snapshot_download\n",
    "# 1. 设置镜像源（国内加速）\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face/\"\n",
    "# os.environ[\"HF_ENDPOINT\"] = \"https://hf-mirror.com\"\n",
    "\n",
    "# 2. 定义模型列表（名称 + 下载路径）\n",
    "# os.path.expanduser(\"./xx\") 会将 ~ 替换为当前用户的主目录\n",
    "models_to_download = [\n",
    "    {\n",
    "        \"repo_id\": \"Qwen/Qwen3-Embedding-0.6B\",  # Embedding 模型 bge-m3 Qwen/Qwen3-Embedding-0.6B\n",
    "        \"local_dir\": os.path.expanduser(\"./mineru_models/Qwen3-Embedding-0.6B\"),\n",
    "    },\n",
    "    {\n",
    "        \"repo_id\": \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # LLM 模型\n",
    "        \"local_dir\": os.path.expanduser(\"./mineru_models/DeepSeek-R1-1.5B\"),\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3. 遍历下载所有模型\n",
    "for model in models_to_download:\n",
    "    while True:  # 断点续传重试机制\n",
    "        try:\n",
    "            print(f\"开始下载模型: {model['repo_id']} 到目录: {model['local_dir']}\")\n",
    "            snapshot_download(\n",
    "                repo_id=model[\"repo_id\"],\n",
    "                local_dir=model[\"local_dir\"],\n",
    "                resume_download=True,  # 启用断点续传\n",
    "                force_download=False,  # 避免重复下载已有文件\n",
    "                token=None,            # 如需访问私有模型，替换为你的 token\n",
    "            )\n",
    "            print(f\"模型 {model['repo_id']} 下载完成！\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"下载失败: {e}, 重试中...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd003b2",
   "metadata": {},
   "source": [
    "* **download hf model**\n",
    "* method-2: AutoTokenizer.from_pretrained(...)\n",
    "<!-- \n",
    "作用：下载 + 加载模型/分词器，并返回可直接使用的 Python 对象。\n",
    "用途：适合快速开发、实验、推理或微调。\n",
    "行为：首次运行会自动下载模型文件到缓存目录；后续运行直接从缓存加载。\n",
    "会检查模型结构、依赖、配置是否正确，并加载权重 \n",
    "-->\n",
    "\n",
    "想直接加载模型做推理或训练：用 AutoTokenizer + AutoModel.from_pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e23faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "# default: c/user/.cache/huggingface/transformers\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)   \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# saved = r\"./my_model\"  \n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=saved, trust_remote_code=True)   \n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=saved)\n",
    "\n",
    "# model.save_pretrained(\"./my_model\")\n",
    "# tokenizer.save_pretrained(\"./my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35b8137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] 使用 HuggingFace 嵌入模型: Qwen/Qwen3-Embedding-0.6B\n",
      "[*] 从本地加载嵌入模型: ..\\temp\\mineru_models\\Qwen3-Embedding-0.6B\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# load local model \n",
    "from pathlib import Path\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "modeln = \"Qwen/Qwen3-Embedding-0.6B\"\n",
    "local_model_dir = r\"..\\temp\\mineru_models\\Qwen3-Embedding-0.6B\"\n",
    "# local_model_dir = r\"D:\\codespace\\fhfeishi\\ragchatzz\\temp\\mineru_models\\Qwen3-Embedding-0.6B\"  # ok\n",
    "\n",
    "print(f\"[*] 使用 HuggingFace 嵌入模型: {modeln}\")\n",
    "            \n",
    "if Path(local_model_dir).exists() and Path(local_model_dir).is_dir():\n",
    "    print(f\"[*] 从本地加载嵌入模型: {local_model_dir}\")\n",
    "    model_path_orn = str(local_model_dir)\n",
    "    # model_path_orn = local_model_dir\n",
    "else:\n",
    "    print(f\"[*] 本地模型路径不存在，尝试从 HuggingFace (或者.cache缓存) 加载: {modeln}\")\n",
    "    model_path_orn = modeln\n",
    "            \n",
    "embed=HuggingFaceEmbeddings(\n",
    "model_name=model_path_orn,  # model_path or model_name\n",
    "model_kwargs={\"device\": 'cpu'},\n",
    "encode_kwargs={\"normalize_embeddings\": True},\n",
    "cache_folder=None,  # 不使用额外缓存\n",
    ")\n",
    "print(embed.cache_folder)  # None \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
