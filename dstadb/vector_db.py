# /dataDB/vector_db.py
#   å°† < å›¾æ–‡markdown > å†™å…¥å‘é‡æ•°æ®åº“ chroma_db
import os, hashlib, re, uuid 
from pathlib import Path
from typing import List, Union
from tqdm import tqdm

# ä½¿ç”¨ Unstructured æ›´å¥½åœ°è§£æ Markdown ä¸­çš„å›¾ç‰‡é“¾æ¥
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores.utils import filter_complex_metadata # è¿‡æ»¤å¤æ‚å…ƒæ•°æ® chromaåªæ”¯æŒ: str int float bool None


class VectorIngestor:
    def __init__(self, config: dict):
        self.config = config
        self.embeddings = self._load_embedding_model()
        self.vectordb = self._init_vectorstore()

    def _load_embedding_model(self):
        modeln = self.config["embed_model"]
        base_url = self.config.get("base_url")
        device = self.config.get("device", "cpu")

        if modeln.startswith("ollama:"):
            model_name = modeln[7:]
            print(f"[*] ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹: {model_name}")
            return OllamaEmbeddings(model=model_name, base_url=base_url)

        else:
            print(f"[*] ä½¿ç”¨ HuggingFace åµŒå…¥æ¨¡å‹: {modeln}")
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ä¸‹åŠ è½½æ¨¡å‹
            local_model_dir = self.config.get("model_dir_local", "")
            if Path(local_model_dir).exists() and Path(local_model_dir).is_dir():
                print(f"[*] ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹: {local_model_dir}")
                model_path_orn = local_model_dir
            else:
                print(f"[*] æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä» HuggingFace (æˆ–è€….cacheç¼“å­˜) åŠ è½½: {modeln}")
                model_path_orn = modeln
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {modeln}")
            return HuggingFaceEmbeddings(
                model_name=model_path_orn,  # model_path or model_name
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True},
                cache_folder=None,  # ä¸ä½¿ç”¨é¢å¤–ç¼“å­˜
            )

    def _init_vectorstore(self) -> Chroma:
        return Chroma(
            collection_name=self.config["collection_name"],
            embedding_function=self.embeddings,
            persist_directory=self.config["store_dir"],
        )

    def load_docs(self, roots: Union[str, Path]) -> List:
        """é€’å½’åŠ è½½ .md æ–‡ä»¶ï¼Œä½¿ç”¨ Unstructured è§£æï¼ˆä¿ç•™å›¾ç‰‡å¼•ç”¨ï¼‰"""
        roots = [Path(r) for r in roots]
        docs = []
        
        for root in roots:
            if not Path(root).exists():
                print(f"[WARN] æ ¹ç›®å½•ä¸å­˜åœ¨: {root}")
                continue
            
            # åŒ¹é… root/*/auto
            for auto_dir in root.glob("*/auto"):
                if not auto_dir.is_dir():
                    print(f"[WARN] è·³è¿‡æ–‡ä»¶ {root}")
                    continue
                
                md_dir = auto_dir  # *.md æ”¾åœ¨ auto/ ç›®å½•ä¸‹
                img_dir = auto_dir / "images"  # å›¾ç‰‡åœ¨ auto/images/
                
                # åŠ è½½æ‰€æœ‰ .md æ–‡ä»¶
                for md_file in md_dir.glob("*.md"):
                    try:
                        loader = UnstructuredMarkdownLoader(str(md_file), mode="elements")  # single 
                        loaded_docs = loader.load()

                        for doc in loaded_docs:
                            source_path = str(md_file.resolve())
                            doc.metadata.update({
                                "source_path": source_path,
                                "file_stem": md_file.stem,
                                "file_suffix": md_file.suffix,
                                "file_size": md_file.stat().st_size,
                                "auto_root": str(auto_dir),  # æ ‡è®°æ¥æº
                            })

                            # æå–å›¾ç‰‡å¼•ç”¨ï¼ˆå¦‚: ![](images/solar.png)ï¼‰
                            image_paths = re.findall(r"!\[.*?\]\((.+?)\)", doc.page_content)
                            if image_paths:
                                resolved_images = []
                                for img_ref in image_paths:
                                    img_ref = img_ref.strip()
                                    # å°è¯•è§£æä¸ºç›¸å¯¹äº auto/ ç›®å½•çš„è·¯å¾„
                                    if img_ref.startswith("images/"):
                                        img_full = (auto_dir / img_ref).resolve()
                                        if img_full.exists():
                                            resolved_images.append(str(img_full))

                                if resolved_images:
                                    doc.metadata["image_paths"] = resolved_images

                        docs.extend(loaded_docs)

                    except Exception as e:
                        print(f"[WARN] è§£ææ–‡ä»¶å¤±è´¥ {md_file}: {e}")


                # # å¯é€‰ï¼šä¹Ÿæ”¯æŒ PDFï¼ˆä½†ä¸å¤„ç†å…¶ä¸­å›¾ç‰‡ï¼‰
                # for fp in root.rglob("*.pdf"):
                #     try:
                #         loader = PyPDFLoader(str(fp))
                #         loaded_docs = loader.load()
                #         for doc in loaded_docs:
                #             doc.metadata["source_path"] = str(fp.resolve())
                #             doc.metadata["file_stem"] = fp.stem
                #             doc.metadata["file_suffix"] = ".pdf"
                #         docs.extend(loaded_docs)
                #     except Exception as e:
                #         print(f"[WARN] è·³è¿‡ PDF {fp}: {e}")

        print(f"[*] æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
        return docs

    def split_docs(self, docs: List, chunk_size: int, chunk_overlap: int) -> List:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        chunks = splitter.split_documents(docs)
        print(f"[*] åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks

    @staticmethod
    def make_id(doc, index: int) -> str:
        # å¤šä¸ªæ–‡æ¡£å—çš„å†…å®¹å¯èƒ½å®Œå…¨ç›¸åŒï¼ˆå°¤å…¶æ˜¯çŸ­æ–‡æœ¬æˆ–é‡å¤æ®µè½ï¼‰ï¼Œå¯¼è‡´ ID é‡å¤ï¼ŒChromaDB æ‹’ç»æ’å…¥é‡å¤ IDã€‚ç”¨uuidéšæœºæ•°ç¼–ç ä¸€ä¸‹
        content = doc.page_content.strip()
        sha = hashlib.sha1(content.encode("utf-8")).hexdigest()[:8]
        stem = Path(doc.metadata.get("source_path", "chunk")).stem
        return f"{sha}_{stem}_{index}_{uuid.uuid4().hex[:4]}"

    def get_existing_ids(self) -> set:
        try:
            return set(self.vectordb.get()["ids"])
        except Exception as e:
            print(f"[INFO] æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡åº“ï¼Œå°†åˆ›å»ºæ–°çš„: {e}")
            return set()

    def ingest(self):
        raw_docs = self.load_docs(self.config["docs_roots"])
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å­—ç¬¦
        raw_docs = filter_complex_metadata(raw_docs)
        if not raw_docs:
            print("[WARN] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç»ˆæ­¢ç´¢å¼•ã€‚")
            return
        
        chunks = self.split_docs(raw_docs, self.config["chunk_size"], self.config["chunk_overlap"])
        
        chunk_ids = [self.make_id(chunk, i) for i, chunk in enumerate(chunks)]

        existing_ids = self.get_existing_ids()
        to_add = [(chunk, cid) for chunk, cid in zip(chunks, chunk_ids) if cid not in existing_ids]

        if not to_add:
            print("[*] æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦ç´¢å¼•ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return

        print(f"[*] æ–°å¢ {len(to_add)} ä¸ªæ–‡æœ¬å—")

        batch_size = self.config["batch_size"]
        for i in tqdm(range(0, len(to_add), batch_size), desc="ğŸ”„ å†™å…¥å‘é‡åº“"):
            batch = to_add[i:i + batch_size]
            docs, ids = zip(*batch)
            self.vectordb.add_documents(documents=docs, ids=list(ids))

        print(f"[âœ…] ç´¢å¼•å®Œæˆï¼æ–°å¢ {len(to_add)} æ¡ï¼Œå‘é‡åº“å­˜å‚¨äº:\n    {self.config['store_dir']}")




if __name__ =="__main__": 
    # config 
    DEFAULT_CONFIG = {
    "docs_roots": [
        r"..\docs.log\zhuanli_RobotFeet",
        r"..\docs.log\zhuanli_RobotHand"
    ],
    "model_dir_local": "../temp/mineru_models/Qwen3-Embedding-0.6B",
    "store_dir": "../docs.log/chroma_db/.zhuanli_vectdb",
    "collection_name": "multi_domain_knowledge",
    "embed_model": "Qwen/Qwen3-Embedding-0.6B",
    "chunk_size": 480,  # 
    "chunk_overlap": 128,
    "batch_size": 16,
    "base_url": "http://localhost:11434",
    "device": "cpu",
}
    ingestor = VectorIngestor(DEFAULT_CONFIG)
    ingestor.ingest()
    
    # å¸¦å›¾ç‰‡çš„markdownè§£ææˆä»€ä¹ˆæ ·äº†ï¼Ÿ  è¿™é‡Œå¯èƒ½é—æ¼äº†å¾ˆå¤šä¿¡æ¯ã€æˆ–è€…è¯´æœ‰éå¸¸å¤šé‡å¤çš„çŸ­ç‰‡æ®µï¼ˆæ®µç‰‡æ®µå“ªæ¥çš„  chunk_size=480å•Šï¼‰
    # åˆ†å—ä¹‹åæ•°æ®æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ  è¿™ä¸ªä¹Ÿå¯ä»¥åç»­æ£€ç´¢æµ‹è¯•éƒ¨åˆ†æŸ¥çœ‹-
    
    
    # temp\mineru_models\models--Qwen--Qwen3-Embedding-0.6B
    # temp/mineru_models/models--Qwen--Qwen3-Embedding-0.6B
    
      













