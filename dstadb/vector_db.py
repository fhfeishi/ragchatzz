# /dataDB/vector_db.py
#   å°† < å›¾æ–‡markdown > å†™å…¥å‘é‡æ•°æ®åº“ chroma_db
import os, hashlib, re, uuid 
from pathlib import Path
from typing import List, Union
from tqdm import tqdm

# ä½¿ç”¨ Unstructured æ›´å¥½åœ°è§£æ Markdown ä¸­çš„å›¾ç‰‡é“¾æ¥
from langchain_community.document_loaders import UnstructuredMarkdownLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores.utils import filter_complex_metadata # è¿‡æ»¤å¤æ‚å…ƒæ•°æ® chromaåªæ”¯æŒ: str int float bool None
from langchain_core.documents import Document
from collections import OrderedDict
from pypdf import PdfReader  # get pubno


class zhuanli_parser:
    def __init__(self, markdown_file: str):
        self.markdown_file = markdown_file
        self.ims_dir = Path(markdown_file).parent / "images"
        self.data_dir = os.path.dirname(markdown_file)
        self.loaded_docs = []
        self.metada_schema = OrderedDict({
            "pubno": str,
            "patent_name": str,
            "applier": str,
            "apply_time": str,
            "fig_list": dict,
        }) # ç”³è¯·å…¬å¸ƒå· ä¸“åˆ©åç§° ç”³è¯·äºº å‘æ˜äºº ç”³è¯·æ—¶é—´ é…å›¾{"å›¾1": [str(description),str(path/to/1.jpg)], }
    
    def __call__(self):
        self.loaded_docs = self._load_with_image()
        
    def _extract_imMatadata(self):
        with open(self.markdown_file, "r", encoding='utf-8') as f:
            content = f.read()
        """ ä»markdownæ–‡æœ¬ä¸­æå–å›¾ç‰‡å…ƒæ•°æ® """
        # 1 æå–â€œé™„å›¾è¯´æ˜â€æ ‡é¢˜åŠå…¶å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜æˆ–æ–‡ä»¶ç»“å°¾ï¼‰
        pattern = re.compile(
            r'^(#{1,3})\s*é™„å›¾è¯´æ˜\s*\n+\s*([\s\S]*?)(?=^#{1,3}|\Z)',
            re.MULTILINE)
        match = pattern.search(content)
        if not match:
            print(f"âš ï¸ markdown file {md_path.stem} æœªæ‰¾åˆ° 'é™„å›¾è¯´æ˜' ç« èŠ‚ï¼Œè·³è¿‡å¤„ç†ã€‚")
            return content

        header_line = match.group(1)  # æ¯”å¦‚ "##"
        body = match.group(2).strip()
        img_map = {}
        # åŒ¹é…ï¼šå›¾ç‰‡è¡Œ + ç´§æ¥ç€çš„â€œå›¾Xâ€è¡Œ
        img_blocks = re.findall(r'!\[.*?\]\((.*?)\)\s*\n\s*å›¾(\d+)', text, re.IGNORECASE)
        for path, num in img_blocks:
            img_map[f"å›¾{num}"] = path.strip()

        # è·å–å›¾ç‰‡çš„å›¾é¢˜ï¼š "å›¾x": [decs, img_path]
        fig_num, desc = body.groups()
        img_path = img_map.get(f"å›¾{fig_num}")
        img_path = os.path.abspath(img_path)
        assert os.path.exists(img_path), f"å›¾ç‰‡è·¯å¾„ä¸å­˜åœ¨: {img_path}"
        if img_path:
            return {"å›¾"+fig_num: [{desc}, str(img_path)]}
        else:
            print(f"âš ï¸ {os.path.basename(self.markdown_file)}æœªæ‰¾åˆ°å›¾{fig_num}çš„è·¯å¾„ï¼Œè·³è¿‡å¤„ç†ã€‚")
            return match.group(0)  # æ— å›¾åˆ™ä¿æŒåŸæ ·
            
            
    
    def _extract_pubno(self):
        # ä¸“åˆ©pdfç¬¬ä¸€é¡µæœ€åä¸€è¡Œ -> ç”³è¯·å…¬å‘Šå·
        pdfp = str(self.markdown_file)[:-3] + ".pdf"
        reader = PdfReader(pdfp)
        text_1 = reader.pages[0].extract_text() or ""
        last_line = text_1.strip().splitlines()[-1]

        # å»æ‰ç©ºæ ¼ååŒ¹é…
        compact = re.sub(r'\s+', '', last_line.upper())
        m = re.search(r'(CN[A-Z0-9]{9,13})', compact)
        if m:
            return m.group(0)
        else:
            raise ValueError(f"ä¸“åˆ© {os.path.basename(pdfp)} æœªæ‰¾åˆ° ç”³è¯·å…¬å‘Šå· çš„å­—ç¬¦ä¸²")    
        
    
    def _load_with_image(self) -> List[Document]:
        with open(self.markdown_file, "r", encoding='utf-8') as f:
            content = f.read()
            image_paths = self._extract_image_paths(content)
            abs_image_paths = [os.path.join(self.data_dir, image_path) for image_path in image_paths]
            
            doc = Document(
                page = content,
                metadata = OrderedDict({
                    "source_path": str(self.markdown_file),
                    "file_stem": os.path.basename(self.markdown_file).split(".")[0],
                    "file_suffix": os.path.basename(self.markdown_file).split(".")[1],
                    "file_size": os.path.getsize(self.markdown_file),
                    "image_paths": abs_image_paths
                })  # 
            ) 
            return [doc]
    def pipeline(self):
        pass 


def get_im_matadata(markdown_text: str):
    """ ä»markdownæ–‡æœ¬ä¸­æå–å›¾ç‰‡å…ƒæ•°æ® """
    
    # 1 æå–â€œé™„å›¾è¯´æ˜â€æ ‡é¢˜åŠå…¶å†…å®¹ï¼ˆç›´åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜æˆ–æ–‡ä»¶ç»“å°¾ï¼‰
    pattern = re.compile(
        r'^(#{1,3})\s*é™„å›¾è¯´æ˜\s*\n+\s*([\s\S]*?)(?=^#{1,3}|\Z)',
        re.MULTILINE)
    match = pattern.search(text)
    if not match:
        print(f"âš ï¸ markdown file {md_path.stem} æœªæ‰¾åˆ° 'é™„å›¾è¯´æ˜' ç« èŠ‚ï¼Œè·³è¿‡å¤„ç†ã€‚")
        return text

    header_line = match.group(1)  # æ¯”å¦‚ "##"
    body = match.group(2).strip()
    img_map = {}
    # åŒ¹é…ï¼šå›¾ç‰‡è¡Œ + ç´§æ¥ç€çš„â€œå›¾Xâ€è¡Œ
    img_blocks = re.findall(r'!\[.*?\]\((.*?)\)\s*\n\s*å›¾(\d+)', text, re.IGNORECASE)
    for path, num in img_blocks:
        img_map[f"å›¾{num}"] = path.strip()

    # è·å–å›¾ç‰‡çš„å›¾é¢˜ï¼š å›¾x-decs
    def repl(match):
        fig_num, desc = match.groups()
        img_path = img_map.get(f"å›¾{fig_num}")
        if img_path:
            return {"å›¾"+fig_num: [{desc}, str(img_path)]}
        else:
            print(f"âš ï¸ {os.path.basename(self.markdown_file)}æœªæ‰¾åˆ°å›¾{fig_num}çš„è·¯å¾„ï¼Œè·³è¿‡å¤„ç†ã€‚")
            return match.group(0)  # æ— å›¾åˆ™ä¿æŒåŸæ ·
        




class VectorIngestor:
    def __init__(self, config: dict):
        self.config = config
        self.embeddings = self._load_embedding_model()
        self.vectordb = self._init_vectorstore()

    def _load_embedding_model(self):
        modeln = self.config["embed_model"]
        base_url = self.config.get("base_url")
        device = self.config.get("device", "cpu")

        if modeln.startswith("ollama:"):
            model_name = modeln[7:]
            print(f"[*] ä½¿ç”¨ Ollama åµŒå…¥æ¨¡å‹: {model_name}")
            return OllamaEmbeddings(model=model_name, base_url=base_url)

        else:
            print(f"[*] ä½¿ç”¨ HuggingFace åµŒå…¥æ¨¡å‹: {modeln}")
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯æœ¬åœ°è·¯å¾„ä¸‹åŠ è½½æ¨¡å‹
            local_model_dir = self.config.get("model_dir_local", "")
            if Path(local_model_dir).exists() and Path(local_model_dir).is_dir():
                print(f"[*] ä»æœ¬åœ°åŠ è½½åµŒå…¥æ¨¡å‹: {local_model_dir}")
                model_path_orn = local_model_dir
            else:
                print(f"[*] æœ¬åœ°æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°è¯•ä» HuggingFace (æˆ–è€….cacheç¼“å­˜) åŠ è½½: {modeln}")
                model_path_orn = modeln
            print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {modeln}")
            return HuggingFaceEmbeddings(
                model_name=model_path_orn,  # model_path or model_name
                model_kwargs={"device": device},
                encode_kwargs={"normalize_embeddings": True},
                cache_folder=None,  # ä¸ä½¿ç”¨é¢å¤–ç¼“å­˜
            )

    def _init_vectorstore(self) -> Chroma:
        return Chroma(
            collection_name=self.config["collection_name"],
            embedding_function=self.embeddings,
            persist_directory=self.config["store_dir"],
        )

    def load_docs(self, roots: Union[str, Path]) -> List:
        """é€’å½’åŠ è½½ .md æ–‡ä»¶ï¼Œä½¿ç”¨ Unstructured è§£æï¼ˆä¿ç•™å›¾ç‰‡å¼•ç”¨ï¼‰"""
        roots = [Path(r) for r in roots]
        docs = []
        
        for root in roots:
            if not Path(root).exists():
                print(f"[WARN] æ ¹ç›®å½•ä¸å­˜åœ¨: {root}")
                continue
            
            # åŒ¹é… root/*/auto
            for auto_dir in root.glob("*/auto"):
                if not auto_dir.is_dir():
                    print(f"[WARN] è·³è¿‡æ–‡ä»¶ {root}")
                    continue
                
                md_dir = auto_dir  # *.md æ”¾åœ¨ auto/ ç›®å½•ä¸‹
                img_dir = auto_dir / "images"  # å›¾ç‰‡åœ¨ auto/images/
                
                # åŠ è½½æ‰€æœ‰ .md æ–‡ä»¶
                for md_file in md_dir.glob("*.md"):
                    try:
                        loader = UnstructuredMarkdownLoader(str(md_file), mode="single")  # single 
                        loaded_docs = loader.load()

                        for doc in loaded_docs:
                            source_path = str(md_file.resolve())
                            doc.metadata.update({
                                "source_path": source_path,
                                "file_stem": md_file.stem,
                                "file_suffix": md_file.suffix,
                                "file_size": md_file.stat().st_size,
                                "auto_root": str(auto_dir),  # æ ‡è®°æ¥æº
                            })

                            # æå–å›¾ç‰‡å¼•ç”¨ï¼ˆå¦‚: ![](images/solar.png)ï¼‰
                            image_paths = re.findall(r"!\[.*?\]\((.+?)\)", doc.page_content)
                            if image_paths:
                                resolved_images = []
                                for img_ref in image_paths:
                                    img_ref = img_ref.strip()
                                    # å°è¯•è§£æä¸ºç›¸å¯¹äº auto/ ç›®å½•çš„è·¯å¾„
                                    if img_ref.startswith("images/"):
                                        img_full = (auto_dir / img_ref).resolve()
                                        if img_full.exists():
                                            resolved_images.append(str(img_full))

                                if resolved_images:
                                    doc.metadata["image_paths"] = resolved_images

                        docs.extend(loaded_docs)

                    except Exception as e:
                        print(f"[WARN] è§£ææ–‡ä»¶å¤±è´¥ {md_file}: {e}")


                # # å¯é€‰ï¼šä¹Ÿæ”¯æŒ PDFï¼ˆä½†ä¸å¤„ç†å…¶ä¸­å›¾ç‰‡ï¼‰
                # for fp in root.rglob("*.pdf"):
                #     try:
                #         loader = PyPDFLoader(str(fp))
                #         loaded_docs = loader.load()
                #         for doc in loaded_docs:
                #             doc.metadata["source_path"] = str(fp.resolve())
                #             doc.metadata["file_stem"] = fp.stem
                #             doc.metadata["file_suffix"] = ".pdf"
                #         docs.extend(loaded_docs)
                #     except Exception as e:
                #         print(f"[WARN] è·³è¿‡ PDF {fp}: {e}")

        print(f"[*] æˆåŠŸåŠ è½½ {len(docs)} ä¸ªæ–‡æ¡£")
        return docs

    def split_docs(self, docs: List, chunk_size: int, chunk_overlap: int) -> List:
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
        )
        chunks = splitter.split_documents(docs)
        print(f"[*] åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—")
        return chunks

    @staticmethod
    def make_id(doc, index: int) -> str:
        # å¤šä¸ªæ–‡æ¡£å—çš„å†…å®¹å¯èƒ½å®Œå…¨ç›¸åŒï¼ˆå°¤å…¶æ˜¯çŸ­æ–‡æœ¬æˆ–é‡å¤æ®µè½ï¼‰ï¼Œå¯¼è‡´ ID é‡å¤ï¼ŒChromaDB æ‹’ç»æ’å…¥é‡å¤ IDã€‚ç”¨uuidéšæœºæ•°ç¼–ç ä¸€ä¸‹
        content = doc.page_content.strip()
        sha = hashlib.sha1(content.encode("utf-8")).hexdigest()[:8]
        stem = Path(doc.metadata.get("source_path", "chunk")).stem
        return f"{sha}_{stem}_{index}_{uuid.uuid4().hex[:4]}"

    def get_existing_ids(self) -> set:
        try:
            return set(self.vectordb.get()["ids"])
        except Exception as e:
            print(f"[INFO] æœªæ£€æµ‹åˆ°ç°æœ‰å‘é‡åº“ï¼Œå°†åˆ›å»ºæ–°çš„: {e}")
            return set()

    def ingest(self):
        raw_docs = self.load_docs(self.config["docs_roots"])
        # è¿‡æ»¤æ‰ä¸æ”¯æŒçš„å­—ç¬¦
        raw_docs = filter_complex_metadata(raw_docs)
        if not raw_docs:
            print("[WARN] æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£ï¼Œç»ˆæ­¢ç´¢å¼•ã€‚")
            return
        
        chunks = self.split_docs(raw_docs, self.config["chunk_size"], self.config["chunk_overlap"])
        
        chunk_ids = [self.make_id(chunk, i) for i, chunk in enumerate(chunks)]

        existing_ids = self.get_existing_ids()
        to_add = [(chunk, cid) for chunk, cid in zip(chunks, chunk_ids) if cid not in existing_ids]

        if not to_add:
            print("[*] æ²¡æœ‰æ–°æ–‡æ¡£éœ€è¦ç´¢å¼•ï¼Œæ— éœ€æ›´æ–°ã€‚")
            return

        print(f"[*] æ–°å¢ {len(to_add)} ä¸ªæ–‡æœ¬å—")

        batch_size = self.config["batch_size"]
        for i in tqdm(range(0, len(to_add), batch_size), desc="ğŸ”„ å†™å…¥å‘é‡åº“"):
            batch = to_add[i:i + batch_size]
            docs, ids = zip(*batch)
            self.vectordb.add_documents(documents=docs, ids=list(ids))

        print(f"[âœ…] ç´¢å¼•å®Œæˆï¼æ–°å¢ {len(to_add)} æ¡ï¼Œå‘é‡åº“å­˜å‚¨äº:\n    {self.config['store_dir']}")




if __name__ =="__main__": 
    # config 
    DEFAULT_CONFIG = {
    "docs_roots": [
        r"..\docs.log\zhuanli_RobotFeet",
        r"..\docs.log\zhuanli_RobotHand"
    ],
    "model_dir_local": r"../deepdocs/mineru_models/Qwen3-Embedding-0.6B",
    "store_dir": r"../docs.log/chroma_db/.zhuanli_vectdb",
    "collection_name": "multi_domain_knowledge",
    "embed_model": "Qwen/Qwen3-Embedding-0.6B",
    "chunk_size": 480,  # 
    "chunk_overlap": 128,
    "batch_size": 16,
    "base_url": "http://localhost:11434",
    "device": "cpu",
}
    ingestor = VectorIngestor(DEFAULT_CONFIG)
    ingestor.ingest()
    
    # å¸¦å›¾ç‰‡çš„markdownè§£ææˆä»€ä¹ˆæ ·äº†ï¼Ÿ  è¿™é‡Œå¯èƒ½é—æ¼äº†å¾ˆå¤šä¿¡æ¯ã€æˆ–è€…è¯´æœ‰éå¸¸å¤šé‡å¤çš„çŸ­ç‰‡æ®µï¼ˆæ®µç‰‡æ®µå“ªæ¥çš„  chunk_size=480å•Šï¼‰
    # åˆ†å—ä¹‹åæ•°æ®æ˜¯æ€ä¹ˆæ ·çš„ï¼Ÿ  è¿™ä¸ªä¹Ÿå¯ä»¥åç»­æ£€ç´¢æµ‹è¯•éƒ¨åˆ†æŸ¥çœ‹-
    
    
    # temp\mineru_models\models--Qwen--Qwen3-Embedding-0.6B
    # temp/mineru_models/models--Qwen--Qwen3-Embedding-0.6B
    
      













